Project Two
Due Oct. 13 at 11:59 PM.

For this first part of the exam, you can either use surveys_complete.csv or your own data. If you are using your own data, you must have data in which you think you have a numerical predictor variable and a numerical response variable. If you are using surveys_complete, you can use weight and hindfoot_length for this.

Save this file in your projects directory. You can either save it in a project two subdirectory, or just put it in projects. Either way is fine.

Load in your data. Which variable will you be using as a predictor, and which as a response? (5 pts)
```{r}
GM<-read_csv("../GM.csv")
#I will use SL (standard length) as the predictor and TV (total length) as the response
```
Plot the two against each other with a scatter plot. Do the data appear to be related linearly? (5 pts)
# Plot here
```{r}
myplot<-ggplot(GM, aes(x=SL, y=TV))+geom_point()
myplot
```
#The data does not appear to be linear.

Fit the linear model. View the summary. (5 pts)
```{r}
model_fit<-lm(TV~SL, data=GM)
summary(model_fit)
```
Does the summary make sense when you compare it to the plot you made? Does our model have good predictive power? Evaluate the residual standard error, intercept, and R-Squared in particular. Would you say your predictor predicts the response? (10 pts)
# Yes, the summary makes sense. No, our model does not have good predictive power. The intercept is around 19.098 which is reasonable looking at the plot, but it has std. error of 0.579. Plus, the SL Pr(>|t|) is 0.3 which is not significant. The residual standard error is 0.8396 which is pretty high and the R-squared values are extremely low (0.02686 and 0.002534). I would say the predictor, standard length does not predict the response total length.

# AMW: That might be the worst R-squared I've ever seen. It's like ... anti-predictive.

Plot the model on the graph. Increase the size of the text so it is comfortably readable at 5 feet. Make sure axis labels are readable and not overlapping with one another. (5 pts)
```{r}
GM_plot1<-ggplot(GM, aes(x=SL, y=TV))+geom_point(size=0.5)+theme(text=element_text(size=15))
GM_plot1<-GM_plot1+geom_smooth(method="lm", color="navy", linewidth=0.5, fill="deeppink4")+annotate("text", x=40, y=30, label= "R^2==0.02686", parse=T)+theme_bw()+labs(x="Standard Length (mm)", y="Total Length (mm)")

```


Check the normality of the residuals. Do they look OK? Are we violating assumptions? (5 pts)
```{r}
model_fit<-lm(SL~TV, data=GM)
broom::augment(model_fit)->augmented_fit
qqnorm(augmented_fit$.resid)
qqline(augmented_fit$.resid, col = "red") 
```
#The residuals are very high and they do not follow the normal line. They do not meet the assumption.

Why is normality of residuals important?

#The normality of residuals is important because it determines if the model fits and/or has bias.


If you are not using surveys_complete: Do you think there are groupings within your data that may have a separate linear model? Try grouping the data by that variable and redoing the lm. If this would not make sense for your data, you may also attempt to do multiple predictor variables. (15 pts)

```{r}
GM2<-GM %>% 
  group_by(Sex)
myplot2<-ggplot(GM2, aes(x=SL, y=TV, color=Sex))+geom_point()
myplot2
model_fit2<-lm(TV~SL, data=GM2)
summary(model_fit2)
GM_plot2<-ggplot(GM2, aes(x=SL, y=TV))+geom_point(size=0.5)+theme(text=element_text(size=15))
GM_plot2<-GM_plot2+geom_smooth(method="lm", color="navy", linewidth=0.5, fill="deeppink4")+annotate("text", x=40, y=30, label= "R^2==0.02686", parse=T)+theme_bw()+labs(x="Standard Length (mm)", y="Total Length (mm)")
model_fit2<-lm(SL~TV, data=GM2)
broom::augment(model_fit)->augmented_fit2
qqnorm(augmented_fit2$.resid)
qqline(augmented_fit2$.resid, col = "red") 

```
#The grouping by sex does not appear to meet the normal residual assumption.




Part Two
In this portion, you are free to use your own data if you have a categorical predictor variable and a response variable. Otherwise, you may use the columns sex and weight in surveys_complete

First, plot the data grouped by sex (5 pts)
```{r}
GM3<-GM %>% 
  group_by(Sex)
ggplot(GM3, aes(x=Sex, y=LL, color=Sex))+geom_jitter()+stat_summary(fun.data="mean_se", color= "black")
model_fit<-lm(LL~Sex, data=GM3)
summary(model_fit)
```
#the variances appear to be equal
#Sex is not a significant predictor of LL (lateral line scale count)

Try an ANOVA of this data (5 pt)

```{r}
aov(model_fit)->anova_model_fit
summary(anova_model_fit)
```
How about a linear model? What information does this give you that an ANOVA can’t? (5 pts)
```{r}
model_fit   <- lm(LL~Sex, data=GM3)
summary(model_fit)
```
#The linear model gives the estimates, std error, t-values, the residuals' quartiles, the residual standard error, adjusted R-squared, multiple R-squared. They both give p value, mean sq, and F-value.

Plot the lm with the data, with points colored by sex. (10 pts)

```{r}
GM3<-GM %>% 
  group_by(Sex)
lm_plot<-ggplot(GM3, aes(x=Sex, y=LL, color=Sex))+geom_jitter()
model_fit<-lm(LL~Sex, data=GM3)
summary(model_fit)

lm_plot+geom_smooth(method="lm", color="navy", linewidth=0.5, fill="deeppink4")+annotate("text", x=2, y=30, label= "R^2==0.02087", parse=T)+theme_bw()

```


Choose any model we’ve looked at so far, but use two predictor variables. Perform an LM, plot the results, and state if this changes your interpretation of the relationship between variables (10 pts)
```{r}
model_fit <- lm(TV ~ Sex + LL, data=GM)
summary(model_fit)


```
 
```{r}
ggplot(GM, aes(x=LL, y=TV, color=Sex))+geom_point(size=2.5)+geom_smooth(method="lm")
```
# The plot shows me the trends better but does not change my opinion that the predictors do not have a relationship with TV.



Part Three
Add and commit this document (5 pts)
#Commit on Git, subject: "project two"
Push your changes to github (10 pts)
#Push

MS students
My expectation is that you’ll do this with your own data. If any part of this doesn’t make sense with your data, please get in touch ASAP so we can work it out.